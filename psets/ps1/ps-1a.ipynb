{
 "metadata": {
  "name": "",
  "signature": "sha256:5577bf6f9ee257dc430459bda6af57787f3ba3bd64cde0e1e6778e7935f708df"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem Set 1a: Text classification\n",
      "==============\n",
      "\n",
      "(_This problem set is graded out of 30 for students taking CS4650 and out of 35 for students taking CS7650. But like all problem sets, this will count towards 6% of your final grade _)\n",
      "\n",
      "\n",
      "In this problem set, you will build a system for classifying movie reviews as positive, negative, or neutral. You will:\n",
      "\n",
      "- Do some basic text processing, tokenizing your input and converting it into a bag-of-words representation\n",
      "- Build a classifier based on sentiment word lists\n",
      "- Build a machine learning classifier, using Naive Bayes\n",
      "- Evaluate your classifiers and examine what they have learned\n",
      "\n",
      "In Problem Set 1b (due one week later), you will try some more advanced classifiers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Installation ##\n",
      "\n",
      "You may need to install some of the libraries below. Usually this is done with pip or easy_install. See here:\n",
      "\n",
      "- [NLTK](http://www.nltk.org/install.html)\n",
      "- [matplotlib](http://matplotlib.org/users/installing.html)\n",
      "- [numpy](http://docs.scipy.org/doc/numpy/user/install.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize, sent_tokenize\n",
      "from collections import defaultdict, Counter\n",
      "from matplotlib import pyplot as plt\n",
      "import numpy as np\n",
      "import scorer\n",
      "import operator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1. Data Processing #\n",
      "(_Completing docsToBOWs() - 4 pts, each question in Deliverable 1 is worth 1 pt. Total 8 pts for part 1_)\n",
      "\n",
      "Your first step is to write code that can apply the following\n",
      "preprocessing steps. You will have to run this code fairly quickly on\n",
      "the test data when you receive it, so make sure it is modular and\n",
      "well-written.\n",
      "\n",
      "- You will edit a function that takes as its argument a \"key\" document.\n",
      "  It should produce a \"BOW\" (bag-of-words) document.\n",
      "  Each line of the key document contains a filename and a label.\n",
      "  Each line of the BOW document should contain a BOW representation of the corresponding\n",
      "  file in the key document. \n",
      "- A BOW representation looks like this: \"word:count word:count word:count...\" for every word that appears in\n",
      "  the document. Do not print words that have zero count. Use space delimiters.\n",
      "- Use NLTK's [tokenization package](http://nltk.org/api/nltk.tokenize.html) function \n",
      "  to divide each file into sentences, and each sentence into tokens.\n",
      "- Downcase all tokens\n",
      "- Only consider tokens that are completely alphabetic.\n",
      "\n",
      "I have provided some shell code, but you will have to fill in the tokenization and filtering steps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def docsToBOWs(keyfile):\n",
      "    with open(keyfile,'r') as keys:\n",
      "        with open(keyfile.replace('.key','.bow'),'w') as outfile:\n",
      "            for keyline in keys:\n",
      "                dataloc = keyline.rstrip().split(' ')[0]\n",
      "                fcounts = ... #add code here\n",
      "                with open(dataloc,'r') as infile:\n",
      "                    for line in infile: \n",
      "                        pass # add code here\n",
      "                for word,count in fcounts.items():\n",
      "                    print >>outfile,\"{}:{}\".format(word,count), #write the word and its count to a line\n",
      "                print >>outfile,\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are the keyfiles that are relevant to this homework. \n",
      "At the beginning you won't have test-imdb.key"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainkey = 'train-imdb.key'\n",
      "devkey = 'dev-imdb.key'\n",
      "testkey = 'test-imdb.key'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, run these lines to produce the BOW files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docsToBOWs(trainkey)\n",
      "docsToBOWs(devkey)\n",
      "# docsToBOWs('test-imdb.key') # you won't have this file yet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next cell defines a [generator function](http://wiki.python.org/moin/Generators), called \"dataIterator\"\n",
      "\n",
      "- This allows you to easily iterate through the dataset defined by a given keyfile. \n",
      "- Each time you call \"next\" (possibly implicitly), it returns a dict containing features and counts for the next document in the sequence. \n",
      "- In this case, the features include the words, and a special \"offset\" feature\n",
      "- This is equivalent to $\\boldsymbol{x}_i$ in the reading.\n",
      "- You can see how this is used in the getAllCounts() function below, which takes a dataIterator as an argument.\n",
      "\n",
      "Lines 7-8 of the code might look confusing if you are not a pythonista. \n",
      "\n",
      "- This is a [list comprehension](http://legacy.python.org/dev/peps/pep-0202/)\n",
      "nested inside a [dict comprehension](http://legacy.python.org/dev/peps/pep-0274/).\n",
      "- Here's an [introduction](http://carlgroner.me/Python/2011/11/09/An-Introduction-to-List-Comprehensions-in-Python.html) with more examples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "offset = '**OFFSET**'\n",
      "def dataIterator(keyfile):\n",
      "    with open(keyfile.replace('key','bow'),'r') as bows:\n",
      "        with open(keyfile,'r') as keys:\n",
      "            for keyline in keys:\n",
      "                textloc,label = keyline.rstrip().split(' ')\n",
      "                fcounts = {word:int(count) for word,count in\\\n",
      "                           [x.split(':') for x in bows.readline().rstrip().split(' ')]}\n",
      "                fcounts[offset] = 1\n",
      "                yield fcounts,label"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataIterator above incrementally re-reads the keyfile and BOW file every time you call it. \n",
      "This is a good idea if you have huge data that won't fit in memory, but the file I/O involves some overhead.\n",
      "If you want, you can write a second dataIterator that iterates across data stored in memory, which\n",
      "will be faster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: How many unique words appear in the training set? (Types, not tokens.) I get 24861. (Don't count the offset feature.)\n",
      "\n",
      "- Note how the dataIterator function is used here. \n",
      "- fcounts is a dict, which it returns for each document.\n",
      "- We are currently ignoring the label, but that is also provided.\n",
      "- This may take a couple of minutes to run"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getAllCounts(datait):\n",
      "    allcounts = Counter()\n",
      "    for fcounts, _ in datait:\n",
      "        allcounts += Counter(fcounts)\n",
      "    return allcounts\n",
      "\n",
      "ac_train = getAllCounts(dataIterator('train-imdb.key'))\n",
      "ac_dev = getAllCounts(dataIterator('dev-imdb.key'))\n",
      "#ac_test = getAllCounts(dataIterator('test-imdb.key'))\n",
      "print \"number of word types\",len(ac_train.keys())-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code makes a plot, with the log-rank (from 1 to the log of the total number of words) \n",
      "on the x-axis and the log count on the y-axis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this enables you to create inline plots in the notebook \n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr_logcounts = np.log(np.array(sorted(ac_train.values(),reverse=True)))\n",
      "plt.plot(np.log(range(len(tr_logcounts))),tr_logcounts)\n",
      "dv_logcounts = np.log(np.array(sorted(ac_dev.values(),reverse=True)))\n",
      "plt.plot(np.log(range(len(dv_logcounts))),dv_logcounts,'r')\n",
      "plt.xlabel('log rank')\n",
      "plt.ylabel('log count')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "<matplotlib.text.Text at 0x3e83a90>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEMCAYAAADNtWEcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XlcVPX6B/DPsIjiLgqkYmqmAqJCGeU6YLhjLrib5lLd\nzNR283bd6uZaLmXdfpkKVq6451KkmIqapqkpuSQYbmgqImLBwPz+eBLFDebMOXNm+bxfr3kxIOc7\nz7zubR7Od3keg9lsNoOIiFyOm94BEBGRPpgAiIhcFBMAEZGLYgIgInJRTABERC6KCYCIyEVplgAG\nDx4MPz8/hISEFPzs8uXLiIqKQt26ddGmTRtkZGRo9fJERFQEzRLAoEGDsHHjxkI/mzx5MqKionDs\n2DG0bt0akydP1urliYioCAYtD4KlpqYiOjoahw4dAgDUr18fW7duhZ+fH86fPw+j0YjffvtNq5cn\nIqIHsOkaQHp6Ovz8/AAAfn5+SE9Pt+XLExHRbTz0emGDwQCDwXDffyMiIstZMqlj0zuAm1M/AHDu\n3Dn4+vre93fNZrPTPsaNG6d7DHx/fG98f873sJRNE0Dnzp0RGxsLAIiNjUWXLl1s+fJERHQbzRJA\nnz590LRpUxw9ehQBAQGYP38+Ro8eje+//x5169bF5s2bMXr0aK1enoiIiqDZGsCiRYvu+fOEhASt\nXtJhGI1GvUPQlDO/P2d+bwDfn6vRdBuoUgaDQdF8FhGRK7P0s5OlIIiIXBQTABGRi2ICICJyUUwA\nREQuSreTwEUZMAAoWdL6R9my8iAiosLsNgE8/TTw11+FH1lZwJ9/3v3zez1u3JCvmZlA+fJAUBAQ\nHFz4a6VKer9LIiL9OP02ULMZSEsDjhwBDh+WrzcfpUrdSgY3E0NoKO8YiMgxWfrZ6fQJ4H7MZuDM\nmcKJ4ddfgUOHgDp1gKZN5fHUU0Dt2gDr0xGRvWMCsFJODrB/P7BzJ5CUBOzYAZhMtxLC4MGAj48u\noRERPRATgMpuTiHt3Als2gR89x0wfz4QFaV3ZEREhTEBaCwhARg0COjRA/jgA9lpRERkD1gKQmNP\nPw388gvwxx9AeLisHxAROSImAAV8fIBly4CRIwGjEZg9W6aKiIgcCaeArHTiBNCvn+wSGjcOaNeO\nO4aISB9cA9BBXh4QHw+8956sCfznP0B0NBMBEdkWE4CO8vOBVauAiRPlw3/ECKBjR+ABrY+JiFTD\nBGAHzGZg7VogLk52DdWrB3TqJAvINWpIQvD01DtKInI2TAB2JicH2LYN+PZb4McfgbNngYsXgYoV\ngTZtgDlzpFYREZG1mAAcQF6eJIH33pODZcuWAY0b6x0VETk6JgAHs2iRrBVMmgQMGcKFYyJSznkS\nQGoq4OEBuLvf/6ubm1N8Yv72m5wsDg0FPvsMKF1a74iIyBE5TwIICJC5EpOp8Nfbn+fnSyJ4UJLw\n9pZaz40a3XrUqiXJw45kZwPDhgF79gBLlgANGugdERE5GudJAMUJy2y+OyncmSiysqTO8y+/AAcO\nyCMjAwgJKZwUQkLs4k/v+fOBN9+UZjUtW8rzevX0joqIHIFrJQClLl8GDh68lRAOHACSk4GAAKB5\nc9mvGRkJ+PlpF8MD5OdLjaE5c6SzWWysLmEQkYNhAlAqN1cm47dulc37iYmyaf/pp+XRsiVQpoxN\nQzp5UnoQnD1rdzNWRGSHmADUYjIBe/cCP/wgCWHPHlmlHTdOEoKN1KsnawLcJkpERWEC0Ep2NrB+\nvZQA7d9fNvGXKKH5y44YAVStCowerflLEZGDYz8ArXh7AzExsph85AjQrBlw/LjmL9uuHbBxo+Yv\nQ0QuiAnAUlWqAGvWAAMHygR9bKymzQBatQJ+/hnIzNTsJYjIRTEBKGEwAMOHy/rA1KlA376ys0gD\npUsDTz0FbN6syfBE5MKYAKzRsKEsFFeuLIfLYmKkHvTff6v6MpwGIiItcBFYLVeuSFeYr76Sg2cx\nMdIVJjQUeOghq0pWJCcDLVrIuvOzz9p8NyoROQjuArIHf/wBfPONzNvs3y+b+Bs3Bpo0Abp3l+cW\nJoStW4FZs6Sk9IIF0l+AiOh2TAD2xmyWk1z79wPbtwNLl0o3mN695Q6hYUOLtpPu3g106QK8/75U\nDyUiuokJwN6ZzXKobPFiOWB24gQQHCzdYSZOlAJ2RTh2TO4ArlwB6teXS6OjgUcftYtyRkSkE4dI\nAJMmTcJXX30FNzc3hISEYP78+fDy8roVlDMngDtdvy5nC/79byAsDPjoo2JdZjYD6emy3LB2LbBp\nE3DqlFSsWLUKKFVK47iJyO7YfQJITU1FZGQkkpOT4eXlhV69eqFDhw4YOHDgraBcKQHcdOWKnCsY\nPhx4+WVFQ+TlAQMGyFCrVtnkoDIR2RG7Pwlcrlw5eHp6Ijs7GyaTCdnZ2ahWrZqtw7A/FStK4+D/\n/hdYt07REO7uskB844aUlSYiehAPW79gpUqV8Prrr6NGjRooVaoU2rZti6fvUVxt/PjxBc+NRiOM\nRqPtgtRL7drAypUyoT9yJPD229LYxgKensCECbJAPHRosZYUiMhBJSYmIjExUfH1Np8C+v333xEd\nHY1t27ahfPny6NGjB2JiYtCvX79bQbniFNDt0tKAQYNkfWDJEilLbQGzWWaT3nhDdp0SkWuw+ymg\nvXv3omnTpvDx8YGHhwe6deuGpKQkW4dh3wICgO++k0/xd9+1+HKDQS576SVpNn/1qgYxEpHDs3kC\nqF+/Pnbt2oUbN27AbDYjISEBQUFBtg7D/rm5AW+9JYXnrl+3+PKOHYEtW+QU8SOPSLN5IqLb6bIN\ndOrUqYiNjYWbmxvCwsIwd+5ceHp63grK1aeAbtehA9CvnzwUOn5czg00by7JoFQpKWZaqZKKcRKR\n7ux+G2hxMAHcZtEiKTltZTW4CxeA//1PdgglJ8u00HffyaIxETkHJgBnk50NVKsmn9r+/qoMmZcH\ndOsmRUvHj5dqFN7eqgxNRDqy+0VgspC3N/DMM3InoBJ3dylc+thjwODBQI8eqg1NRA6EdwCO4Icf\n5FzA3r1AyZKqDp2TI6WIPv5Y+g4QkePiHYAziogAQkKkDOhff6k6dIkSwIcfAq+9BphMqg5NRHaO\ndwCOwmSSbjCXLkmtoJo1JSm4WZ/DzWYgKkq+RkfLDqGKFa0PmYhsi3cAzsrDA1i4UA6HzZ0rHcce\nf1x2B1mZLA0GqU79/PPSsqBGDaB/f6kuyjxM5Lx4B+CozGZgxQo58uvrK/0imzdX5Y4gK0t63c+Y\nAYSHS9sCIrJ/3AbqakwmIC4OmDYNuHxZ6kFPmKDKvk6TSdoZ790LPPywCrESkaY4BeRqPDxkL2dy\nMrBzp7SfDA0FzpxRZejOnYERI6SjZX6+CvESkd3gHYAzGjVKvs6cafVQp0/LksOcOZIQZs6U6hRl\ny1o9NBGpjFNAJHcBDRpI8+DKlVUbdudOuRv47TdpXDZpkmpDE5EKmABIvPAC4Ocni8Mqu3xZFodn\nzpSqo0RkH7gGQGLMGJm7iY9XfehKlaR3ff/+QN++UmCOiBwPE4CzqlkT2LABGDZMSkmoLDoaOHRI\nDiZPmKD68ERkA5wCcnaJiUCvXsAnn0jRH5Wb75w9KweSd+wA6tdXdWgishCngKgwo1Eqvc2bB7Ru\nLd3i16yRWtAqqFpVpoOaNJF6QszbRI6DdwCu5OpVWbn9/nvg2jVgyhQpAuTubvXQV64ArVoBb74p\nJYuIyPa4C4iKZjZLf4Hp04HcXPkTPirK6mF/+QVo00YamEVFybkBIrIdJgAqPrMZWLVK/mzv0UOV\njf0LFwKzZknR0gkTZKeQCuWJiKgYmADIcpcuycGxlSuBJ59UZcjVq4EPPgC8vIBBg6SnfYkSqgxN\nRPfBBEDKrFghq7hJSbKyq4K8PJkOiosDSpWSWacKFVQZmojugbuASJlu3YB//Uu2ifbuLcXlrOTu\nLnXqNm2SfvZt2sg6NBHZByYAumX0aODkSWk006KFdIjJzbV6WC8v2YXapAkQGQmcOKFCrERkNSYA\nKqxSJeCNN6TiW2oqMHasKpv7DQY5jtC/P9Cypcw0EZG+uAZA93fuHNC+PdCpE/D++6oNu2KF9B0e\nPFj61zz2mGpDE7k0rgGQeh56SOoJffopsG4dkJ6uyrDdugF79gDlygHt2sn5ASKyPd4BUNHi4oAF\nC+STumJFqTQ6ZIgqQ0+dKg3pn31W+twHBKgyLJFL4jZQ0o7ZLH+69+gBjBwJvPIK4Olp1ZAmE/DN\nN7JT6NdfJccYDCrFS+RimABIewcPStvJ9HTg//4PaNbM6iFNJqkqGhEhM05EZDmuAZD2GjYEEhIk\nCQwZosqZAQ8PKSm9cqUcGCMi7TEBkDJubsDQodJ6snlz+dM9Lc2qIStVkjWBiROl/zARaYsJgJQz\nGKR8xPHj0oFs/nyrh+zbVw4kd+4M7N9vfYhEdH9cAyB1JCUBPXtKB7I6daweLjYWePllKR8RH8+F\nYaLi4BoA6aNpU+Dtt+Xg2O+/Wz3cwIFSpPTwYeDLL1WIj4juwpYdpJ5XXpHV3IYNAV9fqfkwYoTi\no75eXsBXXwHdu8v20BkzrN51SkS30eUOICMjAzExMQgMDERQUBB27dqlRxikhZdekv6QGzZIZdF2\n7aQSXH6+ouGaNJHZpW3bpJslEalHlzWAgQMHolWrVhg8eDBMJhOuX7+O8uXL3wqKawDOY8cO2S7a\nooW0nlTo6FEZ4maPez8/9UIkchZ2fxDs6tWrCA0NxcmTJ+/7O0wATubSJaBuXWDrVuk8plBWltxg\npKcDGzey1STRnex+ETglJQVVqlTBoEGDEBYWhueffx7Z2dm2DoNsyccH+PBDmQ5avVpxeekyZYDZ\ns6WfgArti4lcXpGLwNu3b0fz5s0L/WzHjh1opvD4v8lkwr59+/DJJ5+gSZMmGDVqFCZPnoyJEycW\n+r3x48cXPDcajTAajYpej+zEc89JIbm335ZdQq+9pmiYihWlnHSrVkDHjkDjxuqGSeRIEhMTkZiY\nqPj6IqeAQkNDsf+OEzn3+llxnT9/Hk899RRSUlIASIKZPHky1q1bdysoTgE5r5MngdBQKSExbZr0\njVRgwgS5mVi/XtpNEpHln533vQPYuXMnkpKScPHiRXz00UcFg167dg35Cnd0AIC/vz8CAgJw7Ngx\n1K1bFwkJCQgODlY8HjmY2rVlRbdVK8DbG3j3XaBkSYuHGTNGGpbVri3J4I03eFiMyFL3XQPIycnB\ntWvXkJeXh2vXriErKwtZWVkoV64cli9fbtWLfvzxx+jXrx8aNWqEgwcPYsyYMVaNRw7G3x9YuFD2\ndtaqJXM6FvL0lMoTycnA118Dr78OZGZqECuREytyCig1NRU1a9a0UTiCU0AuIj8fWLtWiv/MnSuT\n+gqcPy9n0DZvlurU3burHCeRg1B9G+jRo0cxffp0pKamwmQyFbzI5s2brYv0QUExAbiWb7+VlmAt\nWkgp0EaNFA2zd68UJU1OBqpXVzlGIgegegJo2LAhXnrpJYSFhcH9nwU7g8GAxzTs5M0E4ILS06UC\n3KRJMiUUEaFomBEjpB7dli2y+5TIlaieAB577DH8/PPPVgdmCSYAF7Zhg1SCW7VKCsxZyGyWDUan\nTkmbSQ9WuyIXovpBsOjoaMyZMwfnzp3D5cuXCx5EmmjfXhrQt2oFLFtm8eUGA/DZZ0Benmp964mc\nVpF3ADVr1oThHvvrbu7j1yQo3gHQ0qVAr17An38qmsvJzgaCg4HoaNlp6uurQYxEdkb1O4DU1FSk\npKTc9SDSVM+eUvinZk058WUhb29g+3ZpNl+nDjB2rPohEjm6Iu8AYmNj73kHMGDAAO2C4h0A3bRt\nm/SHXLNGdgkpcOaMlJV+4QVg9GhF586IHILqi8DDhw8vSAA3btzA5s2bERYWZvVhsAcGxQRAt/vs\nM2D6dGDRIuCJJxQNcfSolCH66SdpMfnUUyrHSGQHNC8HnZGRgV69emHTpk0WB1dcTABUSF4e8N57\nwDffAAkJQI0aiodatEjWBE6cYOkIcj6al4P29vbmGgDZlrs7MG4c0KeP9BNYs0bxUL17y5pyTIzi\nJmVETqPIO4Do6OiC5/n5+Thy5Ah69uyJKVOmaBcU7wDofrZvB7p0kdPD4eGKhrh2TTqLBQVJw/kS\nJdQNkUgvqk8B3aw1bTAY4OHhgRo1aiAgIMCqIIsMigmAHmTGDGD8eGDlSiAyUtEQFy8C/frJwbH1\n69lsnpyD6lNARqMR9evXR2ZmJq5cuQIvLy+rAiSy2qhRwOefyzmBtWsVDVGlinzwlywJdOgg5waI\nXE2RCWDp0qUIDw/HsmXLsHTpUjzxxBNYpuCEJpFqDAaZzF+5UspGXL2qaBgPD9kR5OMjU0K//qpu\nmET2rljF4BISEuD7z1HKixcvonXr1jh48KB2QXEKiIrrueekW/yXXwLlyysaIjcXmDxZGpT95z/A\nyJFcFyDHpPoUkNlsRpUqVQq+9/Hx4Ycz2Y9Zs2RC/403FDeb9/SUD/4dO4Dly6UtQW6uynES2aEi\nE0C7du3Qtm1bLFiwAPPnz0eHDh3Qvn17W8RGVLTy5WVz/549QN++kgwUCgmRJJCfD7z8sooxEtmp\nYh0Ei4+Px44dOwAALVq0QNeuXbUNilNAZKkbN4BXX5Vy0hs3AoGBioe6ckUOHLdrB8ycqbhvPZHN\nqb4NNCUlBf7+/ihVqhQAKQeRnp6uaZtIJgBSLC4OeOstKSCn8JwAIG0mu3aVG4rvvpPm80T2TvU1\ngJiYmIJOYADg5uaGmJgYZdERaW3AAODTT4EnnwQ++UTxuoC/P5CUJJuN6tQBvv9e5TiJ7ECRCSAv\nLw8lbtsS4eXlhVyukJE969ZNJvM//VSenzqlaBiDAXj/fTlq0K4dsG6dynES6azIBFC5cmWsvq0e\n++rVq1G5cmVNgyKyWtOm0iXex0fKSJ88qXiojh0lCXTpAuzfr2KMRDorcg3gxIkT6NevH86ePQsA\nqF69OhYuXIg6depoFxTXAEhN770HfPCBdIV55x3Fw0ydCowZIzNL//qXivERqUSzctDXrl0DAJQt\nW1ZZZBZgAiDVHT4MREXJ3cCSJYqH2bIFaNsWGD4c+OgjFeMjUoHm/QBsgQmANPHXX8Cjj0oxOSs2\nMly4IC0JuncH5s/nqWGyH5r3AyByWCVLAosXA4MGWdVTwNcXSEkBfvtNdpqeOaNijEQ2xARArqVZ\nM/nwf+YZWRdQ6KGHZKNR7dqy4/TCBRVjJLKRIqeA4uPj72oKX758eYSEhBQUiFM9KE4BkdZOnZLa\nD6tXAxERiocxm+WGYuVKqSMUFaVijEQWUn0NoGPHjti5cyci/vmPJDExEWFhYUhJScHYsWMxYMAA\n6yK+V1BMAGQLK1dKNdElS2Rl14omwQsWAC+8ALRpIzmF5SNID6qvAeTm5iI5ORnx8fGIj4/HkSNH\nYDAYsHv3bk3bQhJprmtXYM4cKSI3cKDiU8OA5JH0dODoUaBlS+DPP9ULk0grRSaAtLQ0+Pn5FXzv\n6+uLtLQ0+Pj4FDohTOSQ+veXFd0ff5TN/Tk5ioeqWPHW2bOqVWWNgMieFZkAIiIi0LFjR8TGxmLB\nggXo3LkzjEYjrl+/jgoVKtgiRiJtlS8PJCTIp3dIiOIOYzeHWrNGSkg0by6HxzibSfaqyDWA/Px8\nrFixoqAcdLNmzdC9e/e7FoZVDYprAKQHs1mmhbZvl6qiHTpYNdy33wI9esgi8cyZbDxP2tPkINj5\n8+exZ88eAEB4eLhmu38KgmICIL2YzcDs2VIyom1bOTfg5aV4uAMHpJBcw4bSpkDDv5uI1F8Evr0p\n/LJly9gUnpybwSBNgVNTgT/+kI3+SUmKh2vUCDh2DNi3T84LWLHEQKQ6NoUnuh+zGRg3TorJzZkD\nDBumeKhr1yQBeHjI4nCZMirGSfQPh2kKn5eXh9DQUERHR1s9FpEmDAZg4kSZu3nlFeDZZxWv6JYt\nC/z0k1zeoIHcFRDpTbem8LNmzUJQUJCmi8lEqmjbFjh+XHpDPvYYcOmSomFKlwZ+/lnOCdSrB8yd\nq3KcRBYqMgFMnToVL774Ig4cOIBDhw7hxRdfxNSpU6160dOnT2P9+vUYOnQop3rIMdSuLUmgcmXp\nF/nrr4qG8fSUDUabNgHPPw/06ydFSon0oEs56B49emDMmDHIzMzE9OnTsXbt2sJBcQ2A7JXZDLz2\nmuzrPHhQzg0odOKElI4wmWQoHqsha1n62elxv38oU6bMfadnDAYDMjMzLY8OwLp16+Dr64vQ0FAk\nJibe9/fGjx9f8NxoNMJoNCp6PSJVGQzST8DTU/Z2zp0LDBmiaKg6dWQtIDJSqouePy8HyYiKKzEx\n8YGfo0Wx+R3AmDFjsHDhQnh4eOCvv/5CZmYmunfvjri4uFtB8Q6AHMGcOdIabPly6Q6jUF4e8Mgj\nQPXqQGKi7BQiUsKhOoJt3bqVU0Dk2MaPByZMkNXdsDDFw6SmArVqAY8/DuzaxWqipIzDdQTjLiBy\naGPHAu3byyf3H38oHqZmTeDsWSlHFB4u6wJEWmNPYCJr5eUBnTvLqu6ePUC5coqHOnkSCAyU8hGL\nFgHe3irGSU7PoaaA7ocJgByOySQ7glJSgCNHZNuoQgcOAI0bAw8/DBw+LOcHiIrD4aaAiJyCh4ec\nDYiMlBXdF18E8vMVDdWokVSkvn5djhykp6scK9E/eAdApLatWwGjEXjiCakJXbmyomFu3ACaNJG7\ngNOngWrV1A2TnA/vAIj01qqVNJ1PSwOCgmSbqIJV3VKlZHNRw4YyzIEDGsRKLo0JgEgLNWrIn+59\n+khXmKZNpe2khby8pD9NixayLsBK7KQmJgAirVSsCMyaJRP6wcFAdLQUArJQ2bLAunVSmLRnT/Ya\nJvUwARBprVw5YN48YMoUqQA3bpxsHbXQf/4ja8tGozznWQGyFheBiWxp7VpgwADgmWeABQsUDbFh\ng/QZbtkS+PxzudEgAngOgMj+3ZzUf+stKSNRsqTFQ/z0E/DSS7JTaOtW4LaeTeTCuAuIyN41bw5s\n2QJ88YWUkDh61OIhnngC2LxZPvirVVN8M0EujgmASA9GoxT/qVpVekSeOmXxEOXLy1//s2fLlBA7\njJGlOAVEpCeTCQgNlW4wsbGKS0hMnQq8/ba0Khg1SuUYyWFwCojIkXh4yMKwhwfw3HOAwkZLb74J\n/O9/kgRee03dEMl58Q6AyB6kpgIdOsiUUFycfFVg/Xqgd29gxAjg/ffVDZHsH+8AiBxRzZpyzDcr\nC4iJkZNfCnToAKxaBXz4oUwHET0I7wCI7MmJE8Ann0gRuaefBj79VPoQW2jBAuD114GuXYHPPpMW\nxuT8VGsKT0Q6qFNHzgY8+aRs9DcYZIK/Vi2Lhnn2WSkh8fLLgNksu4SaN9coZnJYvAMgslerVwPT\npsn0UNeuihrPf/stsHAhcO6c7A7q2lX9MMl+8CQwkTPZuhVYskQ2+W/bJiVBvbwsGuL332VNYMEC\nGSI0FHDj6p9TYgIgckbdu0s56RkzgP79FQ0RHQ0kJgI//CAnicn5cBcQkTOKj5dSoM8/L1VFFVi7\nFujYEYiIAGbOVDk+ckhcBCZyFBMnSqf4Dz+UhsETJ1o8xMKFsivo889lkXjIEA3iJIfBKSAiR3L5\nsqwJjBoF7NsnpSNKlbJoiIsXgY8/BpYuleMGtWtzTcBZcA2AyBV06gTs3g0MHAhMn27x5RkZsi00\nNRVYvFiGI8fHBEDkKuLipN5DZKScHfDzs3iIwYNll9CTTwKTJys6c0Z2hIvARK6iUydpKpOUJNNB\nCrzxhmwqmjULyM5WOT6ye7wDIHJ0L7wgdwPe3pIIata0eIh69YDTp4GwMDkrQI6JdwBEruazz2Rl\nt25daTd5/LjUf7DAoUPAkSO3vp49q1GsZFd4B0DkLEaPBtasAU6elDuBoCCLLjeZpFXx5cvSluDc\nOY3iJM1wEZjI1bVoAfTtK2UjQkMtbjr/999yRmDrVsDf3+I6dKQjTgERubq2bWVNoEsXOTNgIS8v\noF07KUb6zDMaxEd2g3cARM5q5EjAx0cWiStUsPhO4NQpoGlT4OefAXd3oEoVjeIk1fAOgIhEUJA0\nlAkMlH7DFqpcWXJG48ZAtWqSCMi5MAEQOasXXwTOn5dWkxcvWnx56dJySOz8ecBoBC5dUj9E0hcT\nAJGzK18e2LtXTgxHRkpjAAuVKyctJiMjgS+/VD9E0gfXAIicXV4esGOH7PP87jvZ5B8XZ9EQaWly\nvOCHH6R+0NdfaxMqWcfu1wDS0tIQERGB4OBgNGjQALNnz7Z1CESuxd0daNlS/nxv0kSmg9LS5GEy\nFWuIgIB7X56fr3HspCmbJwBPT0/MmDEDhw8fxq5duzBnzhwkJyfbOgwi11S7thz1bdoUCA6WIkAW\nqFULOHpULq9fXyqJkuOyeUMYf39/+Pv7AwDKlCmDwMBAnD17FoGBgbYOhcj1hIbK/k5AKoleuWLR\n5Y0a3br85ZctvpzsjK4dwVJTU7F//36Eh4ff9W/jx48veG40GmE0Gm0XGJEr8PaWshEnT8r3pUoB\nDz1k0eVpabcu9/OTnUNkO4mJiUhMTFR8vW6LwFlZWTAajXj33XfRpUuXwkFxEZhIe+vXA8OH3/o+\nLU3+pC9TpliXx8ZKGwIAyMqS6tTz5mkQJxWbQ9QCys3NRadOndC+fXuMGjXq7qCYAIhsz88POHhQ\nUWOZ5ctlPWD5cg3iomKz+11AZrMZQ4YMQVBQ0D0//IlIJyVLAjdu2PpS0pHN7wC2b9+Oli1bomHD\nhjD8039u0qRJaNeu3a2geAdAZHtBQbIGcHMKqFo1KSVRDFu2AD17yu4gQFpLvv8+0KCBRrHSPTnE\nFFBRmACIdLB/P/DHH/I8Jwd49lngr7+KdenffwPffy9nzgBg5kzpN/zssxrFSvfEBEBE1svPlwNk\n+fmKOsUPHSqN5ocO1SA2ui+7XwMgIgfg5gZ4esqdgAJeXsW+eSAd6XoOgIjsWMmSwJAhkghueu01\nICSkWJdPOpdAAAALLklEQVTGxRUuIT10KNCsmQZxkmJMAER0b0uWSC3om+LipKhcMRLAsGGFF4BX\nrZJCckwA9oUJgIjurX37wt/v31/sKaFHHpHHTWfOcErIHnENgIiKp0QJxWsCVlxKGmICIKLisWJR\n2IpLSUOcAiKi4vHyknoPR44U/nnNmnLqq4hL168H/vyz8M9ffBFo0ULdMKn4eA6AiIrn9GngzsqT\nWVnAv/9dZMPgixeBTZsK/2z5ciAsDBg7Vt0wXRkPghGR7Vy9Ku3CMjMtvnTCBDk5PHGiBnG5KB4E\nIyLb8fAodlvJO3l6Arm5KsdDFmECICLlrPgU9/RUnDtIJUwARKTczTsABVO2Hh68A9AbdwERkXJu\nbvLYvl0+0e9UqRJQr949L/X0lCZkO3fe/W9VqwIPP6xyrHQXLgITkXW6di1cMuImkwn4/Xfg8uV7\nXpaYCLzzzt0/v35dagn99JO6YboC7gIiIvuQlSXtJa9ft+iyX34BBg4EDhzQKC4nxl1ARGQf3N1v\ndYixgIeHostIASYAItKGwgSg8DJSgAmAiLRhRQLg9lDbYAIgIm24ucn20Px8iy7jHYDtMAEQkTYM\nBkWf5kwAtsMEQETaUZgAOAVkGzwIRkTacXcHDh0CSpUq+vfq1QPc3ODuDvz9N/Drrw++pHRpoFYt\n9UJ1RTwHQETaad9ejvsWJSVF6kU3b47sbMBoBLKzH3zJb7/JUYOSJVWJ1ClY+tnJOwAi0s6GDcX7\nvdat5c9+AN7exTsFXKYMp4qsxTUAItKfm5vFu4UUXEJ3YAIgIv0ZDBZ/miu4hO7ABEBE+rt5ZkDb\nS+gOTABEpD9OAemCCYCI9McEoAsmACLSHxOALpgAiEh/TAC6YAIgIv0xAeiCCYCI9McEoAsmACLS\nn8Fg8Z5OBZfQHXRJABs3bkT9+vXx6KOPYsqUKXqEoKvExES9Q9CUM78/Z35vgI7vz0Z3AM7+v5+l\nbJ4A8vLyMHz4cGzcuBFHjhzBokWLkJycbOswdOXs/yd05vfnzO8NYAJwNTZPAD/99BPq1KmDmjVr\nwtPTE71798bq1attHQYR2ROuAejC5tVAz5w5g4CAgILvq1evjt27d9s6DCKyJx4ewPTpwOLFxb7k\ni/PAhSeBy+6Ff/5/DWYj3fvejQKOHQN+/lmelywJLFumNGDnYPN+APHx8di4cSO++OILAMBXX32F\n3bt34+OPP74VlMFgy5CIiJyGXfcDqFatGtJuaxCRlpaG6tWrF/odNoMhItKezdcAHn/8cRw/fhyp\nqanIycnBkiVL0LlzZ1uHQUTk8mx+B+Dh4YFPPvkEbdu2RV5eHoYMGYLAwEBbh0FE5PJ0OQfQvn17\nHD16FCdOnMA777xT6N+c+YxAWloaIiIiEBwcjAYNGmD27Nl6h6S6vLw8hIaGIjo6Wu9QVJeRkYGY\nmBgEBgYiKCgIu3bt0jskVU2aNAnBwcEICQlB37598fc/LRod1eDBg+Hn54eQkJCCn12+fBlRUVGo\nW7cu2rRpg4yMDB0jtM693t+bb76JwMBANGrUCN26dcPVq1cfOIZdnQR29jMCnp6emDFjBg4fPoxd\nu3Zhzpw5TvX+AGDWrFkICgpyyoX8kSNHokOHDkhOTsbBgwed6s41NTUVX3zxBfbt24dDhw4hLy8P\niy3YkWOPBg0ahI0bNxb62eTJkxEVFYVjx46hdevWmDx5sk7RWe9e769NmzY4fPgwDhw4gLp162LS\npEkPHMOuEoCznxHw9/dH48aNAQBlypRBYGAgzp49q3NU6jl9+jTWr1+PoUOHOt1C/tWrV7Ft2zYM\nHjwYgExlli9fXueo1FOuXDl4enoiOzsbJpMJ2dnZqFatmt5hWaVFixaoWLFioZ+tWbMGAwcOBAAM\nHDgQq1at0iM0Vdzr/UVFRcHNTT7Ww8PDcfr06QeOYVcJ4F5nBM6cOaNjRNpJTU3F/v37ER4ernco\nqnn11Vcxbdq0gv8DOpOUlBRUqVIFgwYNQlhYGJ5//nlkZ2frHZZqKlWqhNdffx01atRA1apVUaFC\nBTz99NN6h6W69PR0+Pn5AQD8/PyQnp6uc0TamTdvHjp06PDA37Gr/1KdcdrgXrKyshATE4NZs2ah\nTJkyeoejinXr1sHX1xehoaFO99c/AJhMJuzbtw/Dhg3Dvn37ULp0aYeePrjT77//jpkzZyI1NRVn\nz55FVlYWvv76a73D0pTBYHDaz5z//ve/KFGiBPr27fvA37OrBFCcMwKOLjc3F927d0f//v3RpUsX\nvcNRTVJSEtasWYNatWqhT58+2Lx5MwYMGKB3WKqpXr06qlevjiZNmgAAYmJisG/fPp2jUs/evXvR\ntGlT+Pj4wMPDA926dUNSUpLeYanOz88P58+fBwCcO3cOvr6+OkekvgULFmD9+vXFSuB2lQCc/YyA\n2WzGkCFDEBQUhFGjRukdjqo++OADpKWlISUlBYsXL0ZkZCTi4uL0Dks1/v7+CAgIwLFjxwAACQkJ\nCA4O1jkq9dSvXx+7du3CjRs3YDabkZCQgKCgIL3DUl3nzp0RGxsLAIiNjXWqP8IA2UU5bdo0rF69\nGiVLliz6ArOdWb9+vblu3brmRx55xPzBBx/oHY6qtm3bZjYYDOZGjRqZGzdubG7cuLF5w4YNeoel\nusTERHN0dLTeYajul19+MT/++OPmhg0bmrt27WrOyMjQOyRVTZkyxRwUFGRu0KCBecCAAeacnBy9\nQ7JK7969zQ899JDZ09PTXL16dfO8efPMly5dMrdu3dr86KOPmqOiosxXrlzRO0zF7nx/X375pblO\nnTrmGjVqFHy+vPTSSw8cw+a1gIiIyD7Y1RQQERHZDhMAEZGLYgIgInJRTABERC6KCYCcnr0ctktN\nTS1UuItIb0wA5PS0OO2Zl5en+phEtsYEQC7DbDbjzTffREhICBo2bIilS5cCAPLz8zFs2DAEBgai\nTZs26NixI+Lj4++63mg04tVXX0WTJk0wa9YsrFu3Dk8++STCwsIQFRWFCxcuAADGjx+PwYMHIyIi\nAo888kihdqc3nTx5EmFhYfj5ZoNaIh3YvCEMkV5WrFiBAwcO4ODBg7h48SKaNGmCli1bYvv27Th1\n6hSSk5ORnp6OwMBADBky5K7rDQYDcnNzsWfPHgDSH+BmT4C5c+di6tSpmD59OgDg2LFj2LJlCzIz\nM1GvXj0MGzasYJyjR4+iT58+iI2N5ZQQ6YoJgFzG9u3b0bdvXxgMBvj6+qJVq1bYs2cPduzYgZ49\newKQWjERERH3HaNXr14Fz9PS0tCzZ0+cP38eOTk5qF27NgBJFB07doSnpyd8fHzg6+tbUHXywoUL\n6NKlC1auXIn69etr+G6JisYpIHIZBoPhvpVKi3sgvnTp0gXPX3nlFYwYMQIHDx7E559/jhs3bhT8\nW4kSJQqeu7u7w2QyAQAqVKiAhx9+GNu2bVPyFohUxQRALqNFixZYsmQJ8vPzcfHiRfz4448IDw9H\ns2bNEB8fD7PZjPT0dCQmJt53jNsTRWZmJqpWrQpAKjDe63fuVKJECaxYsQJxcXFYtGiR1e+JyBqc\nAiKnd3MXUNeuXbFz5040atQIBoMB06ZNg6+vL7p3744ffvgBQUFBCAgIQFhY2H27fd2+o2j8+PHo\n0aMHKlasiMjISJw6dargd+6388hgMMDb2xvr1q1DVFQUypYti06dOqn8jomKh8XgiABcv34dpUuX\nxqVLlxAeHo6kpCSnrBVPdDveARAB6NSpEzIyMpCTk4OxY8fyw59cAu8AiIhcFBeBiYhcFBMAEZGL\nYgIgInJRTABERC6KCYCIyEUxARARuaj/B7ylnVTAQgd1AAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x3ea1690>"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1**\n",
      "\n",
      "- Explain what you see in the plot. Does it observe Zipf's law? How well?\n",
      "- Print the token/type ratio for both the training and dev data.\n",
      "- Print the number of types which appear exactly once in the training and dev data\n",
      "- Print the number of types that appear in the dev data but not the training data (hint: use [sets](https://docs.python.org/2/library/sets.html) for this)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(add cells with your answer)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2. Word Lists #\n",
      "(_Completing predict() - 3 pts, setting weights - 2 pts, Deliverable 2 - 1 pt. Total 7 pts for part 2_)\n",
      "\n",
      "- We will now build a sentiment analysis system based on word lists. \n",
      "- The file \"sentiment-vocab.tff\" contains a sentiment lexicon from [ Wilson et al 2005](http://people.cs.pitt.edu/~wiebe/pubs/papers/emnlp05polarity.pdf). \n",
      "- The code below reads the lexicon into memory, building sets of positive and negative words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poswords = set()\n",
      "negwords = set()\n",
      "with open('sentiment-vocab.tff','r') as fin:\n",
      "    for i,line in enumerate(fin):\n",
      "        # more list and dict comprehensions!\n",
      "        kvs = {key:val for key,val in [kvp.split('=') for kvp in line.split() if '=' in kvp]}\n",
      "        if kvs['type'] == 'strongsubj':\n",
      "            if kvs['priorpolarity'] == 'negative':\n",
      "                negwords.add(kvs['word1'])\n",
      "            if kvs['priorpolarity'] == 'positive':\n",
      "                poswords.add(kvs['word1'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, you should write a classifier that classifies each instance in a testfile. The classification rule is:\n",
      "\n",
      "- 'POS' if the instance has more words from the positive list than the negative list\n",
      "- 'NEG' if the instance has more words from the negative list than the positive list\n",
      "- 'NEU' (neutral) if the instance has the same number of words from each list\n",
      "\n",
      "To do this, you will write a function \"predict\", \n",
      "which represents the inner-product computation $\\boldsymbol{\\theta}' \\boldsymbol{f}(\\boldsymbol{x},y)$.\n",
      "It should have the following characteristics:\n",
      "\n",
      "- **Input 1** an instance, represented as a dict (with features as keys and counts as values) \n",
      "- **Input 2** a dictionary of weights, where keys are tuples of features and labels, and weights are the values. This corresponds to $\\boldsymbol{\\theta}$ in the reading. See example below.\n",
      "- **Input 3** a list of possible labels\n",
      "- **Output 1** the highest-scoring label\n",
      "- **Output 2** a dict with labels as keys and scores as values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use this to find the highest-scoring label\n",
      "argmax = lambda x : max(x.iteritems(),key=operator.itemgetter(1))[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(instance,weights,labels):\n",
      "    scores = # your code here\n",
      "    # return the highest-scoring label, and the scores for all labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are weights for the simplest classifier, which simply labels all instances as positive.\n",
      "\n",
      "Note that it uses only the 'offset' feature"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_labels = ['POS','NEG','NEU']\n",
      "weights_all_pos = defaultdict(int)\n",
      "weights_all_pos.update({('POS',offset):1,('NEG',offset):0,('NEU',offset):0})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is some code for evaluating your classifiers. \n",
      "It uses a scoring library that I wrote, and writes the output to a file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evalClassifier(weights,outfilename,testfile=devkey):    \n",
      "    with open(outfilename,'w') as outfile: #open the output file\n",
      "        for counts,label in dataIterator(testfile): #iterate through eval set\n",
      "            print >>outfile, predict(counts,weights,all_labels)[0] #print prediction to file\n",
      "    return scorer.getConfusion(testfile,outfilename) #run the scorer on the prediction file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below shows how to evaluate this classifier. \n",
      "\n",
      "- **Sanity check**: You should get 40.7% accuracy just by classifying everything as positive. This is the \"most common class\" (MCC) baseline.\n",
      "\n",
      "The printed output is a **confusion matrix**. \n",
      "The rows indicate the key and the columns indicate the response. \n",
      "In this case, the response is always \"POS\", so there is only one column. \n",
      "The cell NEG/POS tells you how often an example that was labeled \"NEG\" in the key was labeled \"POS\" in the system response."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mat = evalClassifier(weights_all_pos,'all_pos.txt')\n",
      "print scorer.printScoreMessage(mat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 classes in key: set(['NEG', 'NEU', 'POS'])\n",
        "1 classes in response: set(['POS'])\n",
        "confusion matrix\n",
        "key\\response:\tPOS\n",
        "NEG\t\t401\t\n",
        "NEU\t\t192\t\n",
        "POS\t\t407\t\n",
        "----------------\n",
        "accuracy: 0.4070 = 407/1000\n",
        "\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now build a classifier based on the word lists. The classifier should have the following decision rule:\n",
      "\n",
      "- If the number of positive words (tokens) is greater than the number of negative words, choose label 'POS'\n",
      "- If the number of negative words (tokens) is greater than the number of positive words, choose label 'NEG'\n",
      "- If they are equal, choose label 'NEU'\n",
      "\n",
      "You should manually set the weights to ensure this behavior; don't change the predict function at all.\n",
      "You'll need to use the offset weights to make sure that ties go to the 'NEU' label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2**: run your classifier on dev.key, and use the following code to print the resulting confusion matrix.\n",
      "\n",
      "The confusion matrix should now have three columns, since the response should include every class at least once. The count of correct responses is found on the diagonal of the confusion matrix. What is the most frequent type of error?\n",
      "\n",
      "**Sanity check**: The accuracy should be 55.9%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mat = evalClassifier(weights_list,'word_list.txt')\n",
      "print scorer.printScoreMessage(mat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(include your explanation of the most frequent type of error here)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3. Naive Bayes #\n",
      "(_Completing learnNBWeights() - 5 pts, Deliverable 3a - 1pt, 3b - 1 pt, explanation of plot output - 2pts. Total 8 points for part 3_)\n",
      "\n",
      "Now you will implement a Naive Bayes classifier.\n",
      "\n",
      "You already have the code for the decision function, \"predict\". \n",
      "So you just need to construct a set of weights that correspond to the classifier. \n",
      "These weights will contain two parameters:\n",
      "\n",
      "- $\\log \\mu$ for the offset, which parametrizes the prior $\\log P(y)$\n",
      "- $\\log \\phi$ for the word counts, which parametrizes the likelihood $\\log P(x | y)$\n",
      "\n",
      "You should use maximum *a posteriori* estimation of\n",
      "the parameter $\\phi$,\n",
      "$$\\phi_{j,n} = P(w = n | y = j) = \\frac{\\sum_{i: y_i = j} x_{i,n} + \\alpha}{\\sum_{i:y_i=j} \\sum_{n'} x_{i,n'} + V\\alpha}$$\n",
      "where \n",
      "\n",
      "- $y_i = j$ indicates the class label $j$ for instance $i$\n",
      "- $w=n$ indicates word $n$\n",
      "- $\\alpha$ is the smoothing parameter\n",
      "- $V$ is the total number of words\n",
      "\n",
      "For each class, normalize by the sum of counts of words **in that class**. In other words, $\\sum_j \\phi_{j,n} = 1$ for all $j$. You can estimate $\\log \\phi$ directly if you prefer.\n",
      "\n",
      "For the prior $\\log P(y)$, you can use relative frequency estimation.\n",
      "\n",
      "Both probabilities should be estimated from the training data only.\n",
      "Please write this code yourself -- do not use other libraries, and try to do\n",
      "it without looking at other code online."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import chain #hint, especially if you're obsessive about being pythonic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the word counts first, because this can be slow\n",
      "# you may also wish to keep a list of all word types that are observed in the training data\n",
      "counts = defaultdict(lambda : Counter()) # hint\n",
      "class_counts = defaultdict(int) # hint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should write a *function* to compute the weights for a given value of $\\alpha$, \n",
      "because you will want to vary this value later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learnNBWeights(alpha=0.1):\n",
      "    weights =     # your code here\n",
      "    return weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3a**\n",
      "Train a classifier from the training data, and apply it to\n",
      "the development data, with $\\alpha = 0.1$. Report the confusion matrix and the accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the weights\n",
      "weights_nb = learnNBWeights(alpha=0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: For the following instance, I get the scores shown."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict({'good':1,'worst':4,offset:1},weights_nb,all_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "('NEG',\n",
        " {'NEG': -27.17069677176088,\n",
        "  'NEU': -33.079364309993927,\n",
        "  'POS': -36.719119167364646})"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this code to evaluate your weights\n",
      "mat = evalClassifier(weights_nb,'nb.txt')\n",
      "print scorer.printScoreMessage(mat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3b** Try at least seven different values of $\\alpha$. Plot the accuracy on both the dev and training sets for each value, using [subplot](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot) to show two plots in the same cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr_accs = []\n",
      "dv_accs = []\n",
      "alphas = # your choice\n",
      "weights_nb_alphas = dict()\n",
      "for alpha in alphas:\n",
      "    print alpha,\n",
      "    # learn the weights\n",
      "    weights_nb_alphas[alpha] = learnNBWeights(alpha) \n",
      "    # evaluate on training data\n",
      "    confusion = evalClassifier(weights_nb_alphas[alpha],'nb.alpha.tr.txt',trainkey)\n",
      "    tr_accs.append(scorer.accuracy(confusion))\n",
      "    # evaluate on dev data\n",
      "    confusion = evalClassifier(weights_nb_alphas[alpha],'nb.alpha.dv.txt',devkey)\n",
      "    dv_accs.append(scorer.accuracy(confusion))\n",
      "    print dv_accs[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this code to plot the accuracies\n",
      "subplot(1,2,1)\n",
      "plot(log(alphas),tr_accs,'bx-')\n",
      "ylabel('training accuracy')\n",
      "subplot(1,2,2)\n",
      "plot(log(alphas),dv_accs,'rx-')\n",
      "ylabel('dev. accuracy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Use this cell to explain what you see in the plot above)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4. Feature Analysis #\n",
      "\n",
      "(_Completing  getTopFeats() - 2 pts, Deliverable 4a - 1pt, 4b - 2 pts, 4c - 2 pts, 4d -5pts . Total 7 pts for CS4650 and 12 pts for CS7650_)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4a**\n",
      "What are the words that are most predictive of positive versus negative text?\n",
      "You can measure this by $\\log \\phi_{pos,n} - \\log \\phi_{neg,n}$ (which is similar to the [likelihood ratio test](http://en.wikipedia.org/wiki/Likelihood-ratio_test)).\n",
      "Use $\\alpha = 1$ from the dev data.\n",
      "\n",
      "List the top five words and their counts for each class. Do the same for the top 5 words that predict negative versus positive.\n",
      "\n",
      "Consider using [operator.itemgetter()](http://docs.python.org/2.7/library/operator.html) for easily sorting dictionaries by their values. See my definition of the argmax function above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getTopFeats(weights,class1,class2,K=5):\n",
      "    # your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this\n",
      "print getTopFeats(weights_nb_alphas[1],'POS','NEG')\n",
      "print getTopFeats(weights_nb_alphas[1],'NEG','POS')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4b** Now do the same thing for $\\alpha = 100$. Which words look better to you? \n",
      "Which gave better accuracy? \n",
      "Explain what you think is going on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this\n",
      "print getTopFeats(weights_nb_alphas[100],'POS','NEG')\n",
      "print getTopFeats(weights_nb_alphas[100],'NEG','POS')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(explain here)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4c** Consider the weights for $\\alpha=100$. \n",
      "\n",
      "- Print words $w$ that are in the positive lexicon, but for which $\\log \\phi_{neg,w} > \\log \\phi_{pos,w} + 0.1$. \n",
      "    - (These words are more likely in the negative class, despite being in the positive lexicon.)\n",
      "- Print words $w$ that are in the negative lexicon, but for which the $\\log \\phi_{pos,w} > \\log \\phi_{neg,w} + 0.1$. \n",
      "    - (These words are more likely in the positive class, despite being in the negative lexicon.)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4d** (7650 only)\n",
      "\n",
      "What do you think is going on here? Pick one of these words, and look for example reviews that contain it (using [grep](http://en.wikipedia.org/wiki/Grep)). \n",
      "\n",
      "Is the word used in the opposite sense, or is there some other explanation?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(explain here)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}