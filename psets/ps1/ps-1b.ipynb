{
 "metadata": {
  "name": "",
  "signature": "sha256:c760865a4f70ee42440a86b271a26e054b1b7d1ea9bc28ab3ee68ccb9e42fa1e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem Set 1b: Discriminative classification\n",
      "==========\n",
      "\n",
      "- In this problem set, you will build on your code from Part 1a, this time implementing perceptron and logistic regression classifiers. \n",
      "- You'll want to copy in code from the first pset, as needed.\n",
      "- This problem set is scored out of 30 points for CS4650 students, 33 points for CS7650. It counts for six points towards your grade.\n",
      "- Read all the way through before starting, so you can budget your time appropriately."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scorer\n",
      "import operator\n",
      "from collections import defaultdict, Counter\n",
      "import matplotlib.pyplot as plt\n",
      "%pylab --no-import-all inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainkey = 'train-imdb.key'\n",
      "devkey = 'dev-imdb.key'\n",
      "testkey = 'test-imdb.key'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "offset = '**OFFSET**'\n",
      "def dataIterator(keyfile):\n",
      "    with open(keyfile.replace('key','bow'),'r') as bows:\n",
      "        with open(keyfile,'r') as keys:\n",
      "            for keyline in keys:\n",
      "                textloc,label = keyline.rstrip().split(' ')\n",
      "                fcounts = {word:int(count) for word,count in\\\n",
      "                           [x.split(':') for x in bows.readline().rstrip().split(' ')]}\n",
      "                fcounts[offset] = 1\n",
      "                yield fcounts,label"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for speed, it's better to just read in all the training instances, since they will fit in memory\n",
      "all_tr_insts = []\n",
      "for inst,label in dataIterator(trainkey):\n",
      "    all_tr_insts.append((inst,label))\n",
      "all_dev_insts = []\n",
      "for inst,label in dataIterator(devkey):\n",
      "    all_dev_insts.append((inst,label))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use this to find the highest-scoring label\n",
      "argmax = lambda x : max(x.iteritems(),key=operator.itemgetter(1))[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# paste your predict function from pset 1a here\n",
      "# you are free to modify it if you can improve it\n",
      "# as before, you should return two outputs: the highest-scoring label, and the scores for all labels\n",
      "def predict(instance,weights,labels):\n",
      "    # your code here\n",
      "    return argmax(scores),scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_labels = ['POS','NEG','NEU']\n",
      "def evalClassifier(weights,outfilename,testfile=devkey):    \n",
      "    with open(outfilename,'w') as outfile:\n",
      "        for counts,label in dataIterator(testfile): #iterate through eval set\n",
      "            print >>outfile, predict(counts,weights,all_labels)[0] #print prediction to file\n",
      "    return scorer.getConfusion(testfile,outfilename) #run the scorer on the prediction file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 5. Perceptron #\n",
      "\n",
      "Implement a perceptron classifier. Using the feature-function\n",
      "representation, include features for each word-class pair, and also an\n",
      "``offset'' feature for each class. Given a set of word counts $\\vec{x}_i$,\n",
      "a true label $y_i$, and a guessed label $\\hat{y}$, your update will be\n",
      "\\begin{align*}\n",
      "\\hat{y} & \\leftarrow \\text{argmax}_y \\vec{\\theta}' f(\\vec{w}_i,y)\\\\\n",
      "\\vec{\\theta} & \\leftarrow \\vec{\\theta} + f(\\vec{w}_i, y_i) - f(\\vec{w}_i, \\hat{y}).\n",
      "\\end{align*}\n",
      "\n",
      "Please write this yourself -- do not use any libraries, and try not to look\n",
      "at other code online.\n",
      "\n",
      "**Sanity check** If you are not careful, learning can be slow. \n",
      "You may need to think a little about how to do this update efficiently. \n",
      "\n",
      "- On my laptop, I can make 10 passes on the training data in roughly 30 seconds, including evaluating the accuracy on the dev and training sets. \n",
      "- You can use the '%%timeit' cell magic to compute statistics like this.\n",
      "- Your code doesn't have to be as fast as mine, but it needs to be written intelligently, and it needs to be fast enough for you to debug it properly.\n",
      "- The '%%prun' cell magic is also useful for diagnosing speed"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5a** (5 points)\n",
      "\n",
      "Implement a function that runs the perceptron for a single iteration (one pass through the training data), using the shell below. Signature:\n",
      "\n",
      "- **Input 1**: training set generator: an object that enables you to iterate through a dataset, such as all_tr_insts\n",
      "- **Input 2**: a dictionary of weights, representing the current classifier at the time you call this function\n",
      "- **Input 3**: a list of all possible labels\n",
      "- **Output 1**: the weights after training\n",
      "- **Output 2**: the number of training errors\n",
      "- **Output 3**: the number of training instances\n",
      "\n",
      "The second and third outputs allow you to compute the *training set accuracy*. This way, you can see whether you are overfitting or underfitting."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' your code '''\n",
      "def oneItPerceptron(data_generator,weights,labels):\n",
      "    tr_err = 0\n",
      "    for i,(counts,label) in enumerate(data_generator):\n",
      "        pass # replace with your code\n",
      "    return weights, tr_err, i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this code trains the perceptron for N iterations on the supplied training data\n",
      "def trainPerceptron(N_its,inst_generator=all_tr_insts,labels=all_labels):\n",
      "    tr_acc = [None]*N_its #holder for training accuracy\n",
      "    dv_acc = [None]*N_its #holder for dev accuracy\n",
      "    weights = defaultdict(float) \n",
      "    for i in xrange(N_its):\n",
      "        weights,tr_err,tr_tot = oneItPerceptron(inst_generator,weights,labels) #call your function for a single iteration\n",
      "        confusion = evalClassifier(weights,'perc.txt') #evaluate on dev data\n",
      "        dv_acc[i] = scorer.accuracy(confusion) #compute accuracy\n",
      "        tr_acc[i] = 1. - tr_err/float(tr_tot) #compute training accuracy from output\n",
      "        print i,'dev: ',dv_acc[i],'train: ',tr_acc[i] \n",
      "    return weights, tr_acc, dv_acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5b** (1 point): Train your classifier on trainkey for ten iterations, and plot the output, using the code in the cells below. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w_perc,tr_acc_perc,dv_acc_perc = trainPerceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this code makes plots of the training and development set accuracy\n",
      "def makePlots(tr_acc,dv_acc):\n",
      "    ax1 = plt.subplot(1,2,1,xlabel='iteration',ylabel='accuracy')\n",
      "    plt.plot(tr_acc,'rx-')\n",
      "    plt.title('training')\n",
      "    plt.subplot(1,2,2,xlabel='iterator',sharey=ax1)\n",
      "    plt.plot(dv_acc,'bx-')\n",
      "    plt.title('development')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "makePlots(tr_acc_perc,dv_acc_perc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check** Your training set accuracy should increase quickly, but your dev set accuracy might be disappointing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 6. Averaged Perceptron #\n",
      "\n",
      "Notice how the dev set performance of the perceptron was very unstable. Now you will try to improve it using averaging.\n",
      "\n",
      "Conceptually, the idea is to keep a running total of the weights, and then divide at the end, after $T$ updates:\n",
      "\n",
      "\\begin{align*}\n",
      "\\hat{y} & \\leftarrow \\text{argmax}_y \\theta' f(\\vec{w}_i,y)\\\\\n",
      "\\theta^t & \\leftarrow \\theta^{t-1} + f(\\vec{w}_i, y_i) - f(\\vec{w}_i, \\hat{y})\\\\\n",
      "\\overline{\\theta} & = \\frac{1}{T} \\theta^T\n",
      "\\end{align*}\n",
      "\n",
      "Then you can use $\\overline{\\theta}$ to make predictions.\n",
      "\n",
      "But in practice, this is very inefficient. You can't store the weights after every update -- it's much too big. But you don't want to compute a running sum either. The reason is that the weight vector will quickly become dense, and this would require $O(\\#F)$ operations at every update, where $\\#F$ is the number of features. This is much more work than the standard perceptron update, which only involves the features that are active in the current instance. In a bag-of-words model, each document will typically have only a small fraction of the total vocabulary, and we would like each update to be linear in the number of features active in the document, not the total number of features.\n",
      "\n",
      "An efficient solution was pointed out by [Daume 2006](http://hal3.name/docs/daume06thesis.pdf). \n",
      "Let $\\delta_t$ indicate the update at time $t$.\n",
      "Then, assuming $\\theta^0 = 0$, we have:\n",
      "\n",
      "\\begin{align*}\n",
      "\\theta^t = & \\theta^{t-1} + \\delta_t \\\\\n",
      "= & \\sum_{t' < t} \\delta_{t'}\n",
      "\\end{align*}\n",
      "\n",
      "We would like to compute the sum of the weight vectors,\n",
      "\\begin{align*}\n",
      "\\sum_t^T \\theta_t = & \\sum_t^T \\sum_{t' \\leq t} \\delta_{t'} = T \\delta_0 + (T-1) \\delta_1 + (T - 2) \\delta_2 + \\ldots + \\delta_T \\\\ \n",
      "= & \\sum_t^T (T - t) \\delta_t\\\\\n",
      "= & T \\sum_t^T \\delta_t - \\sum_t^T t \\delta_t \\\\\n",
      "= & T \\theta_t - \\sum_t^T t \\delta_t \\\\\n",
      "\\frac{1}{T} \\sum_t^T \\theta_t = & \\theta_T - \\frac{1}{T} \\sum_t^T t \\delta_t\n",
      "\\end{align*}\n",
      "\n",
      "This means we need to keep another running sum, $\\sum_t^T t \\delta_t$, the sum of scaled updates. \n",
      "To compute the average, we divide by the number of updates $T$ and subtract it from the current weight vector."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 6a** (5 points) Implement averaged perceptron, using two functions\n",
      "\n",
      "- an outer loop, trainAvgPerceptron, which should have the same inputs and outputs as my trainPerceptron above\n",
      "- an inner loop, oneItAvgPerceptron, which makes a single pass through the training data. To do weight averaging, this function may have to take some additional arguments and offer some additional outputs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' you should add code within this function'''\n",
      "def trainAvgPerceptron(N_its,inst_generator=all_tr_insts,labels=all_labels):\n",
      "    tr_acc = [None]*N_its\n",
      "    dv_acc = [None]*N_its\n",
      "    weights = defaultdict(float)\n",
      "    wsum = defaultdict(float)\n",
      "    for i in xrange(N_its):\n",
      "        # You will define oneItAvgPerceptron below. Call it here.\n",
      "        weights,wsum,tr_err,i = oneItAvgPerceptron(inst_generator,weights,wsum,labels,t)\n",
      "        # When you are done with this training pass, compute the averaged weights, as shown above.\n",
      "        \n",
      "        avg_weights = defaultdict(float)\n",
      "        confusion = evalClassifier(avg_weights,'perc.txt')\n",
      "        dv_acc[i] = scorer.accuracy(confusion)\n",
      "        tr_acc[i] = 1. - tr_err/float(tr_tot)\n",
      "        print i,'dev:',dv_acc[i],'train:',tr_acc[i]\n",
      "    return weights, tr_acc, dv_acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Your function oneItAvgPerceptron should be similar to oneItPerceptron, but it needs to take additional arguments to keep track of the running sum of weights, and the total number if instances seen. \n",
      "It also needs to output this information."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' you should add code within this function '''\n",
      "def oneItAvgPerceptron(inst_generator,weights,wsum,labels,t_init=0):\n",
      "    tr_err = 0\n",
      "    for i,(counts,label) in enumerate(inst_generator):\n",
      "        # your code here\n",
      "    return weights, wsum, tr_err, i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 6b** (1 point): Train your classifier on trainkey for ten iterations, and plot the output, using the code in the cells below. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# again, this takes roughly 30 seconds for me\n",
      "w_ap,tr_acc_ap,dv_acc_ap = trainAvgPerceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "makePlots(tr_acc_ap,dv_acc_ap)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check** the dev set performance should be much better than the non-averaged perceptron"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 6c** (2 points) Use your getTopFeats function from pset 1a to compute the top five features for positive and negative classes, by contrasting the weights $\\theta_{pos,n} - \\theta_{neg,n}$ and $\\theta_{neg,n} - \\theta_{pos,n}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print getTopFeats(w_ap,'POS','NEG',K=5)\n",
      "print getTopFeats(w_ap,'NEG','POS',K=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 7. Logistic regression #"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.misc import logsumexp #hint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 7a** (3 points): implement a function to compute the normalized probability of each label.\n",
      "\n",
      "- This function should have the same input arguments as your predict function\n",
      "- It should output a dict, from labels to probabilities\n",
      "- It will need to be fast. You may need to optimize this later. As always, %%prun and %%timeit are your friends."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the normalized probability of each label \n",
      "def computeLabelProbs(instance,weights,labels):\n",
      "    # your code\n",
      "    return {label:prob for ...}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**sanity check**: your code should reproduce my results below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights = defaultdict(float)\n",
      "weights[('NEG','bad')] = 1\n",
      "weights[('NEG','best')] = -1\n",
      "weights[('POS','bad')] = -0.5\n",
      "weights[('POS','best')] = 2\n",
      "weights[('NEU',offset)] = 3\n",
      "computeLabelProbs({'bad':1,'best':2,offset:1},weights,all_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "{'NEG': 0.0068674111043921151,\n",
        " 'NEU': 0.37494794181688146,\n",
        " 'POS': 0.61818464707872656}"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 7b** (4 points) Now implement logistic regression, training by stochastic gradient descent.\n",
      "\n",
      "- I've provided an outline of the code, including the regularization\n",
      "- You need to provide the code that computes the likelihood gradient for this instance\n",
      "- My best implementation is still pretty slow; it takes around 15 seconds per pass through the training set\n",
      "- Unlike the perceptron code, you can do everything within the single function trainLRbySGD\n",
      "- For a reminder about how SGD works, see my notes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainLRbySGD(N_its,inst_generator=all_tr_insts,learning_rate=1e-4,regularizer=1e-2):\n",
      "    w_dict = defaultdict(float)\n",
      "    dv_acc = [None]*N_its\n",
      "    tr_acc = [None]*N_its\n",
      "\n",
      "    # this block is all to take care of regularization\n",
      "    ratereg = learning_rate * regularizer #gratuitous optimization\n",
      "    def regularize(base_feats,t):\n",
      "        for base_feat in base_feats:\n",
      "            for label in all_labels:\n",
      "                w_dict[(label,base_feat)] *= (1 - ratereg) ** (t-last_update[base_feat])\n",
      "            last_update[base_feat] = t\n",
      "\n",
      "    for it in xrange(N_its):\n",
      "        tr_err = 0\n",
      "        last_update = defaultdict(int) # reset, since we regularize at the end of every iteration\n",
      "        for i,(inst,true_label) in enumerate(inst_generator):\n",
      "            # apply \"just-in-time\" regularization to the weights for features in this instance\n",
      "            regularize(inst,i)\n",
      "            \n",
      "            # YOUR CODE HERE: compute likelihood gradient from this instance, and update weights\n",
      "\n",
      "        # regularize all features at the end of each iteration\n",
      "        regularize([base_feature for label,base_feature in w_dict.keys()],i)\n",
      "        \n",
      "        dv_acc[it] = scorer.accuracy(evalClassifier(w_dict,'sgd'))\n",
      "        tr_acc[it] = 1. - tr_err/float(i)\n",
      "        print it,'dev:',dv_acc[it],'train:',tr_acc[it]\n",
      "    return w_dict,tr_acc,dv_acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 7c ** (1 point): run this code to see how it works."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w_sgd,tr_acc_sgd,dv_acc_sgd = trainLRbySGD(10,regularizer=1e-2)\n",
      "makePlots(tr_acc_sgd,dv_acc_sgd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 7d ** (1 point): try two other regularizers, one greater than 1e-2, and one less. \n",
      "\n",
      "- print the same output.\n",
      "- store the weights separately"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 7e** (1 point): compute the top five features for each set of weights (from each regularizer), using your getTopFeats function. Explain what the resulting feature weights show about the effect of regularization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print getTopFeats(w_sgd,'POS','NEG',K=5)\n",
      "print getTopFeats(w_sgd,'NEG','POS',K=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Extra credit ** (2 points): My implementation of regularization looks pretty different from how it is explained in the reading, but I claim that my regularizer is functionally identical. What's going on? Can you explain why I'm right or why I'm wrong?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 8. Making it better #\n",
      "\n",
      "In this independent component, will improve your classifiers from either pset 1a or 1b. There are two general paths for improving these classifiers: data and algorithms.\n",
      "\n",
      "- Data-oriented approaches relate to the features. For example, you could try to use bigrams, remove stopwords, lemmatize (using wordnet), etc. Or, you could try non-linear feature combinations such as the presence of pairs of words, etc.\n",
      "- Algorithm-oriented approaches relate to the learning itself. For example, you could implement Passive-Aggressive, AdaGrad (described in my notes), feature hashing (see [this paper](http://alex.smola.org/papers/2009/Weinbergeretal09.pdf)), alternative regularizers, or various improvements to naive bayes (see [this paper](http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf)). Note that not all these approaches will improve accuracy; some will improve speed. You may want to look into some of the supplemental reading to get ideas for improvements.\n",
      "- Students in 4650 should try one improvement of either type; students in 7650 should try one improvement of each type.\n",
      "\n",
      "**Deliverable 8** (3 points for 4650; 6 points for 7650): Clearly explain what you did, and why you thought it would\n",
      "work. Do an experiment to test whether it works. Creativity and thoughtfulness counts more than raw performance here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 9. Bakeoff! #\n",
      "\n",
      "48 hours before the assignment is due, I will send you unlabeled test\n",
      "data. Your job is to produce a response file that I can evaluate. I'll \n",
      "present the results in class and give the best scorers a chance to explain\n",
      "what they did.\n",
      "\n",
      "** Deliverable 10 ** (3 points) Run your best system from any part of the\n",
      "assignment on the test data. Recall that evalClassifier() produces a response file. Rename this file \n",
      "lastname-firstname.response, and include it in your submission on T-Square. (Please get this \n",
      "filename right, otherwise we may miss your submission to the bakeoff.) The top scores will be announced in\n",
      "class.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}