# Deliverable 1.3

Why do you think the type-token ratio is lower for the dev data as compared to the training data?

(Yes the dev set is smaller; why does this impact the type-token ratio?)
The types of the vocabulary in dev data are similar to the training data and are about same count, but the total count of tokens in dev data is smaller than the training data. So the type-token ratio is lower for the dev data as compared to the training data.
In general, considering the edge cases such that training data contains all vocabularies in the world. Then the dev data can at most has same amount of types of vocabularies as the trainig set. Then the denominator for type-token ratio is fixed, and the total word counts of dev set is smaller than the training set, so the type-token ratio is lower for the dev data as compared to the training data set.


# Deliverable 3.5

Explain what you see in the scatter plot of weights across different smoothing values.
With smaller smoothing value, the theta values tend to be smaller than the ones with larger smoothing value. Because when taking log of close to 0, such as 0.01, would give a really smalle value such as -15.

# Deliverable 6.2

Now compare the top 5 features for logistic regression under the largest regularizer and the smallest regularizer.
Paste the output into ```text_answers.md```, and explain the difference. (.4/.2 points)

Relatively small regularizer: theta_lr_best,theta_lr_hist_best = logreg.estimate_logreg(x_tr,y_tr,20,0.001,regularizer = 0.0001)
[((u'worldnews', u'plane'), 0.49679132971040957),
 ((u'worldnews', u'ai'), 0.49107741018921475),
 ((u'worldnews', u'ukraine'), 0.49084321023341049),
 ((u'worldnews', u'russia'), 0.48876059453906812),
 ((u'worldnews', u'russian'), 0.46283938272988595)]
[((u'science', u'research'), 0.49680862429376799),
 ((u'science', u'ebv'), 0.44129964896934526),
 ((u'science', u'study'), 0.43051243225384905),
 ((u'science', u'corn'), 0.38015492798233896),
 ((u'science', u'evolution'), 0.36259235626703706)]
[((u'iama', u'gun'), 0.34697065663059595),
 ((u'iama', u'thanks'), 0.32925328460489811),
 ((u'iama', u'state'), 0.32719376234894137),
 ((u'iama', u'marijuana'), 0.32662111523051252),
 ((u'iama', u'request'), 0.29332393349804914)]
[((u'todayilearned', u'hr'), 0.42698734284685858),
 ((u'todayilearned', u'latin'), 0.33984772161228644),
 ((u'todayilearned', u'apple'), 0.3398013329487039),
 ((u'todayilearned', u'bear'), 0.27820435044335451),
 ((u'todayilearned', u'ancient'), 0.23101576991510508)]
[((u'askreddit', u'porn'), 0.35885371181544062),
 ((u'askreddit', u'one'), 0.25446497919150779),
 ((u'askreddit', u'go'), 0.2376738994715093),
 ((u'askreddit', u'some'), 0.23562941530478815),
 ((u'askreddit', u'night'), 0.2320116672582169)]
Relatively large regularizer: theta_lr_best,theta_lr_hist_best = logreg.estimate_logreg(x_tr,y_tr,20, 0.001, 1)
[((u'worldnews', '**OFFSET**'), 0.30969492716705277),
 ((u'worldnews', u'russia'), 0.032910591568721635),
 ((u'worldnews', u'ukraine'), 0.032121711971211712),
 ((u'worldnews', u'the'), 0.031301331284294326),
 ((u'worldnews', u'.'), 0.030621273367017938)]
[((u'science', '**OFFSET**'), 0.20299974074214136),
 ((u'science', u'is'), 0.054639048129339049),
 ((u'science', u'of'), 0.029744650633251272),
 ((u'science', u','), 0.027828293441810849),
 ((u'science', u'can'), 0.020828340799120076)]
[((u'iama', '**OFFSET**'), 0.41540858457836355),
 ((u'iama', u'you'), 0.074822399247919766),
 ((u'iama', u'i'), 0.07074629120413399),
 ((u'iama', u'!'), 0.055012427614358018),
 ((u'iama', u'have'), 0.036477784826463397)]
[((u'todayilearned', u'it'), 0.026351340919404331),
 ((u'todayilearned', u"''"), 0.025771326673496259),
 ((u'todayilearned', u'a'), 0.024255410742459099),
 ((u'todayilearned', u"'s"), 0.023202388330923657),
 ((u'todayilearned', u'``'), 0.021625510558329133)]
[((u'askreddit', u'i'), 0.034813454055633554),
 ((u'askreddit', u','), 0.030582592825770083),
 ((u'askreddit', u'my'), 0.029886770632019017),
 ((u'askreddit', u'*'), 0.028937420658167949),
 ((u'askreddit', u'try'), 0.028426129005195186)]
 For smaller regularizer, the weights for some frequent words in the training set can be relatively large, and the model would be a little bit overfit to the training data and with higher bias and higher variance. However, for larger regularizer, the weights for the "offset" features are much larger than the weights for words in the training set, which prevents the model to overfit the training data and introduce lower bias as well as lower variance to make the model to be generalized for any kind of testing data.

# Deliverable 7.2

Explain the new preprocessing that you designed: why you thought it would help, and whether it did.
I removed all the punctuations in the sentence, because when I checked top features, some top features are ".", which is meaningless as a feature.

# Deliverable 8

Describe the research paper that you have chosen.

- What are the labels, and how were they obtained?
- Why is it interesting/useful to predict these labels?  
- What classifier(s) do they use, and the reasons behind their choice? Do they use linear classifiers like the ones in this problem set?
- What features do they use? Explain any features outside the bag-of-words model, and why they used them.
- What is the conclusion of the paper? Do they compare between classifiers, between feature sets, or on some other dimension? 
- Give a one-sentence summary of the message that they are trying to leave for the reader.

N/A