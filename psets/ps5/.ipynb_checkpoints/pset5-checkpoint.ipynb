{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem set 5: Parsing\n",
    "===========\n",
    "\n",
    "This problem set contains two parts. \n",
    "\n",
    "- ** English Dependency parsing **: design features to learn a high-accuracy dependency parser for the English language\n",
    "- ** Multilingual Dependency Parsing **: design features to learn a high-accuracy dependency parser for a language other than english of your choice\n",
    "\n",
    "### Honor policy ###\n",
    "\n",
    "- Your work must be your own. Do not discuss the details of the assignment with other people. \n",
    "- You may of course help each other with understanding the ideas discussed in lecture and the readings, and with basic questions about programming in Python. It is **not acceptable** to discuss how to implement a specific feature for dependency parsing, and it is unacceptable to share your code.\n",
    "- There are implementations and source code for many machine learning algorithms on the internet. Please write the code for this assignment on your own, without using these external resources, except where noted (usually in the bakeoff only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: English Dependency parsing #\n",
    "\n",
    "In this problem, you will work with an arc-factored non-projective dependency parser, which is trained by average perceptron. If you were not confident about your own implementation of averaged structure perceptron, please take a look at the code, in the directories shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import join\n",
    "import gtparsing\n",
    "import gtparsing.dependency_parser as depp\n",
    "import gtparsing.dependency_features as depf\n",
    "import gtparsing.custom_features\n",
    "import gtparsing.utilities\n",
    "from score import accuracy\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = \"data/deppars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a dependency parser with a given feature set\n",
    "dp = depp.DependencyParser(feature_function=depf.DependencyFeatures())\n",
    "dp.read_data(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train your parser for ten iterations\n",
    "# These are *unlabeled* accuracies.\n",
    "dp.train_perceptron(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In file ```parsing/custom_features.py```, you are going to create a series of subclasses of ```DependencyFeatures```, which has features of your choice. We provide an example of one such class ```LexFeats```, which adds a feature that includes:\n",
    "\n",
    "- The part-of-speech of the head\n",
    "- The word of the modifier\n",
    "\n",
    "** Note ** - Please take a careful look at the documentation provided for the class. It should help you design the other classes for the subsequent deliverables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(gtparsing.custom_features)\n",
    "from gtparsing.custom_features import LexFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's run it\n",
    "dp = depp.DependencyParser(feature_function=LexFeats())\n",
    "dp.read_data(\"english\")\n",
    "dp.train_perceptron(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 1a ** (3 points): start by adding a feature that computes the distance between the head and the modifier, up to a maximum absolute value of 10. To do this, implement the code for the class ```LexDistFeats``` in ```gtparsing.custom_features```\n",
    "\n",
    "**Sanity check**: you should now have 23019 features. Accuracy should improve substantially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload (gtparsing.custom_features)\n",
    "from gtparsing.custom_features import LexDistFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser(feature_function=LexDistFeats())\n",
    "dp.read_data(\"english\")\n",
    "dp.train_perceptron(10)\n",
    "dp.test(join (DIR, \"english_dev.conll\"), join (DIR, \"deliverable1a.conll\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 1b ** (3 points): now add a feature to LexDistFeats, which includes the POS of the modifier and the word of the head. To do this, implement the code for the class ```LexDistFeats2``` in ```gtparsing.custom_features```\n",
    "\n",
    "**Sanity check**: The number of features should roughly double. (Do you see why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload (gtparsing.custom_features)\n",
    "from gtparsing.custom_features import LexDistFeats2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser(feature_function=LexDistFeats2())\n",
    "dp.read_data(\"english\")\n",
    "dp.train_perceptron(5)\n",
    "dp.test(join (DIR, \"english_dev.conll\"), join (DIR, \"deliverable1b.conll\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context features ## \n",
    "\n",
    "Add context features that consider the tags adjacent to the head and modifier. You may wish to consider various tag combinations, such as \n",
    "\n",
    " - $\\langle t[h], t[h-1], t[m]\\rangle$: head, head-left, modifier\n",
    " - $\\langle t[h], t[m], t[m+1]\\rangle$: head, modifier, modifier-right\n",
    " - $\\langle t[h], t[h-1], t[m], t[m+1]\\rangle$: head, head-left, modifier, modifier-right\n",
    " - etc\n",
    "\n",
    "Note that you can add more than one feature at a time within ```create_arc_features()```. Watch out for edge cases!\n",
    "\n",
    "** Deliverable 1c ** (5 points):\n",
    "\n",
    "Describe what context feature templates you have added. How do they impact the total number of features? How does \n",
    "it impact the development and training accuracy?\n",
    "\n",
    "** Note **- You must atleast add head, head-left, modifier feature\n",
    "\n",
    "To do this, implement the code for the class ```ContextFeats``` in ```gtparsing.custom_features```\n",
    "\n",
    "** Sanity check **: I added a few basic context features, and this increased dev set accuracy above 82%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload (gtparsing.custom_features)\n",
    "from gtparsing.custom_features import ContextFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser(feature_function=ContextFeats())\n",
    "dp.read_data(\"english\")\n",
    "dp.train_perceptron(10)\n",
    "dp.test(join (DIR, \"english_dev.conll\"), join (DIR, \"deliverable1c.conll\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your response to Deliverable 1c here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bakeoff! ##\n",
    "\n",
    "Similar to the previous problem sets, we will have a kaggle competition for this problem set. Two days before the submission, we will release a test data set. Try to develop other features that will further improve performance. Please explain all the features that you have. We will provide the submission function through piazza.\n",
    "\n",
    "After identifying your best feature set, run *test()* function from DependencyParser. Please name the file as **lastname-firstname.conll.pred**.\n",
    "\n",
    "**Deliverable 1d** (4 points): Explain why the features you added would work and include your response file in your t-square submission.\n",
    "\n",
    "**Sanity check / challenge**: The best test set performance in Fall 2014 was 86.5%. Can you beat it??\n",
    "\n",
    "**NOTE**: As usual, you are not supposed to look at other code online while doing this problem set. However, you are welcome to search for *research papers* on dependency parsing to get ideas for features that might improve your performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Uncomment the line below and modify it so that the filename is lastname-firstname.conll.pred\n",
    "#DELIVERABLE1d = join (DIR, \"soni-sandeep.conll.pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run this when you have trained on your best features. \n",
    "dp.test(join (DIR, \"english_dev.conll\"), DELIVERABLE1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Multilingual Dependency parsing #\n",
    "\n",
    "In this part, you will do a series of tasks:\n",
    "\n",
    "- **Delexicalized direct transfer learning**. This means training a parser without lexical features, and directly porting it to another languag, without retraining.\n",
    "- Retrain the delexicalized parser in the target language.\n",
    "- Train a lexicalized parser in the target language.\n",
    "\n",
    "** Note ** - Running each part of this section can be slow, so make sure you start early if you want to finish in time.\n",
    "\n",
    "First, you choose which target language you want to work with. Uncomment the line from the following cell depending on the language you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TARGET_LANGUAGE = \"german\"\n",
    "#TARGET_LANGUAGE = \"french\"\n",
    "#TARGET_LANGUAGE = \"italian\"\n",
    "#TARGET_LANGUAGE = \"spanish\"\n",
    "#TARGET_LANGUAGE = \"portuguese\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get us started, we warmup by comparing the probability distributions of part of speech tags for two different datasets.\n",
    "\n",
    "- In part 1, you used the dataset identified by \"english\" language. \n",
    "- The same data is also available for \"english_univtags\" language. \n",
    "- The difference is in the size of the tagset, where the size is smaller in english_univtags. The tagset is chosen such that it has _universal_ applicability to multiple languages. Effectively, the tagging is much more coarse grained in english_univtags than in english.\n",
    "\n",
    "** Deliverable 2a ** (3 points): Calculate the conditional probability distribution of modifier PoS tag given head PoS tag i.e $P(\\text{modifier tag}=n \\mid \\text{head tag}=m), \\forall n \\in T$, where $T$ is the tagset.  In order to do this, implement the ```CPT``` function in ```gtparsing.utilities```, which should take as arguments a list of instances and the tag index representing the head of a dependency relation, and should return a dict of probabilities of the tags of the modifiers.\n",
    "\n",
    "Use this function to calculate the conditional probability distributions where the head tag is verb for \"english\" and \"english_univtags\" datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload (gtparsing.utilities)\n",
    "from gtparsing.utilities import CPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser(feature_function=depf.DependencyFeatures())\n",
    "dp.read_data(\"english\")\n",
    "eng_tag_dist = CPT(dp.reader.train_instances, dp.reader.pos_dict[\"VB\"])\n",
    "eng_reverse_lookup = {value:key for key,value in dp.reader.pos_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser(feature_function=depf.DependencyFeatures())\n",
    "dp.read_data(\"english_univtags\")\n",
    "eng_univ_tag_dist = CPT(dp.reader.train_instances, dp.reader.pos_dict[\"VERB\"])\n",
    "eng_univ_reverse_lookup = {value:key for key,value in dp.reader.pos_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_dist (values, labels):\n",
    "    WIDTH = 0.35\n",
    "    f = figure (figsize = (10,5))\n",
    "    ax = f.add_subplot (111)\n",
    "    ax.bar (np.arange (len(values)),values)\n",
    "    xticks (np.arange (len (values)) + WIDTH, labels, rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_dist (eng_tag_dist.values(), [eng_reverse_lookup[tag_index] for tag_index in eng_tag_dist.keys()])\n",
    "plot_dist (eng_univ_tag_dist.values(), [eng_univ_reverse_lookup[tag_index] for tag_index in eng_tag_dist.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2b ** (3 points) : Now calculate the [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) of each of the distribution. How are the two distributions different? \n",
    "\n",
    "In order to do this, you will implement ```entropy``` function ```gtparsing.utilities``` module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload (gtparsing.utilities)\n",
    "from gtparsing.utilities import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print entropy (eng_tag_dist)\n",
    "print entropy (eng_univ_tag_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2c ** (3 points) Implement ```DelexicalizedFeats``` in ```gtparsing/custom_features```. You should have the following features:\n",
    "\n",
    "- part of speech tag of the head and modifier pair\n",
    "- distance between the head and the modifier upto a maximum absolute value of 10\n",
    "\n",
    "Train the dependency parser on ** english_univtags ** and transfer learning to the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload (gtparsing.custom_features)\n",
    "from gtparsing.custom_features import DelexicalizedFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training on english\n",
    "dp = depp.DependencyParser(feature_function=DelexicalizedFeats())\n",
    "dp.read_data(\"englishu\")\n",
    "dp.train_perceptron(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# applying to the target language\n",
    "new_dp = depp.DependencyParser(feature_function=DelexicalizedFeats())\n",
    "new_dp.read_data(TARGET_LANGUAGE)\n",
    "new_dp.setWeights (dp.weights)\n",
    "new_dp.test(join (DIR, TARGET_LANGUAGE+\"_dev.conll\"), join (DIR, TARGET_LANGUAGE + \".deliverable2c.conll\"))\n",
    "print \"Accuracy:\", accuracy(join (DIR, TARGET_LANGUAGE+\"_dev.conll\"), join (DIR, TARGET_LANGUAGE + \".deliverable2c.conll\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2d **(3 points) Train the delexicalized parser in the target language. \n",
    "\n",
    "* Find the worst predicted label i.e the label for which the parser makes the most mistakes.\n",
    "* Find the best predicted label ie. the label for which the parser makes the least mistakes,\n",
    "\n",
    "In order to do this you may want to compare the predictions to the true labels by calculating the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser(feature_function=DelexicalizedFeats())\n",
    "dp.read_data(TARGET_LANGUAGE)\n",
    "dp.train_perceptron(10)\n",
    "dp.test(join (DIR, TARGET_LANGUAGE+\"_dev.conll\"), join (DIR, TARGET_LANGUAGE + \".deliverable2d.conll\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 2e** (3 points) Now train the parser for 10 iterations using lexical features. Explain the following:\n",
    "\n",
    "* Have the mistakes for the worst predicted label from 2d gone down? \n",
    "* Give top 3 worst predicted labels and top 3 best predicted labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser(feature_function=) # Make appropriate call here\n",
    "dp.read_data(TARGET_LANGUAGE)\n",
    "dp.train_perceptron(10)\n",
    "dp.test(join (DIR, TARGET_LANGUAGE+\"_dev.conll\"), join (DIR, TARGET_LANGUAGE + \".deliverable2e.conll\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2f** (6 points) Train the parser for 10 iterations to improve the accuracy of the parser by at least 3%. Include at least one context feature and one morphological feature. The morphological features should access the words themselves; this is possible using the ```self.word_dict``` member variable of ```dependency_features```. Try to include more than one feature of each kind. Explain what features you added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser(feature_function=) # Make appropriate call here\n",
    "dp.read_data(TARGET_LANGUAGE)\n",
    "dp.train_perceptron(10)\n",
    "dp.test(join (DIR, TARGET_LANGUAGE+\"_dev.conll\"), join (DIR, TARGET_LANGUAGE + \".deliverable2f.conll\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: 7650 Only #\n",
    "\n",
    "- This is more research-oriented than in previous assignments. Be prepared to read and to think!\n",
    "- [Hohensee and Bender](http://aclweb.org/anthology/N/N12/N12-1032.pdf) show that features measuring morphological  agreement can significantly improve dependency parsing.\n",
    "- For your target language, read about morphology in your chosen target language.\n",
    "- Design a set of features that capture morphological agreement between the head and the modifier. These features can condition on the part-of-speech, but they need to take the morphology into account too. For example, in English, you might want to capture subject-verb agreement. To do this you could design the following features:\n",
    "    - < pos(h) = V, pos(m) = N, head verb is third-person singular, modifier noun is third-person singular>\n",
    "    - < pos(h) = V, pos(m) = N, head verb is not third-person singular, modifier noun is plural>\n",
    "    - etc\n",
    "- In the example above, you could use simple morphology to distinguish whether the noun is plural and whether the verb agrees, for example by checking for 's' at the end of each word.\n",
    "- If you can find existing resources (dictionaries or code) that capture morphology of interest for your targeted language, you may use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Deliverable 3** (8 points) \n",
    "\n",
    "- Explain the morphological agreement that your features are trying to capture.\n",
    "- Explain how you try to implement morphological agreement as a feature.\n",
    "- Test whether your feature improves performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
